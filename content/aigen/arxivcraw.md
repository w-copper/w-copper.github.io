+++
date = '2025-07-24T16:59:31+08:00'
draft = false
title = 'å¦‚ä½•æŠ“å– arXiv è®ºæ–‡'
+++
<!--more-->
# å¦‚ä½•æŠ“å– arXiv è®ºæ–‡

## ä¸€ã€ç®€ä»‹

arXiv æ˜¯ä¸€ä¸ªå¼€æ”¾è·å–çš„å­¦æœ¯è®ºæ–‡é¢„å°æœ¬å¹³å°ï¼Œæ¶µç›–ç‰©ç†ã€æ•°å­¦ã€è®¡ç®—æœºç§‘å­¦ç­‰å¤šä¸ªé¢†åŸŸã€‚é€šè¿‡ç¼–ç¨‹æ–¹å¼ï¼Œæˆ‘ä»¬å¯ä»¥æ–¹ä¾¿åœ°æŠ“å– arXiv ä¸Šçš„è®ºæ–‡ä¿¡æ¯ã€‚

---
### ä½¿ç”¨ arXiv çš„å®˜æ–¹ API

#### âœ… API åœ°å€ï¼š
```
https://export.arxiv.org/api/query
```

#### ğŸ“Œ å‚æ•°è¯´æ˜ï¼š
- `search_query`ï¼šæœç´¢å…³é”®è¯ï¼ˆä¾‹å¦‚ `"cs.AI"`ï¼‰
- `start`ï¼šèµ·å§‹ä½ç½®ï¼ˆç”¨äºåˆ†é¡µï¼‰
- `max_results`ï¼šæœ€å¤§è¿”å›æ•°é‡ï¼ˆæœ€å¤š100ï¼‰

#### ğŸ“¦ ç¤ºä¾‹ä»£ç ï¼ˆPythonï¼‰ï¼š

```python
import requests
from feedparser import parse

url = 'https://export.arxiv.org/api/query'
params = {
    'search_query': 'cs.AI',
    'start': 0,
    'max_results': 5
}

response = requests.get(url, params=params)
feed = parse(response.text)

for entry in feed.entries:
    print("æ ‡é¢˜:", entry.title)
    print("æ‘˜è¦:", entry.summary)
    print("é“¾æ¥:", entry.link)
    print("å‘å¸ƒæ—¥æœŸ:", entry.published)
    print("-" * 50)
```

#### ğŸ“š å®‰è£…ä¾èµ–ï¼š
```bash
pip install requests feedparser
```
